Implement Memory Functions (memory.js):

SQLite Functions: Create async functions to:

Get or create a user profile.

Add a new message to the chat_history table.

Retrieve the last N messages for a user.

Update the user's profile with new facts.

LanceDB Functions: Create async functions to:

addMemory(userId, textChunk, embedding): Store a text embedding in LanceDB with userId as metadata. The embedding itself can be generated via the Gemini API.

fetchRelevantMemories(userId, queryVector, nResults=3): Query LanceDB to find the most similar memories for a specific userId.

Step 3: LLM Handler (src/chatbot/llm_handler.js) (3-5 Hours)
Import and initialize the Google Generative AI client using your API key from .env.

Create a function to get embeddings for text: async function getEmbedding(text). This will be used to create vectors for LanceDB.

Create the main response generation function: async function generateResponse(context).

Prompt Engineering: Inside this function, use JavaScript template literals to build the master prompt.

JavaScript

// Example Prompt Structure in JS
const systemPrompt = `
You are Stan, a witty, empathetic, and friendly AI companion...
---
CONTEXT ABOUT THE USER (User ID: ${context.userId}):
- Profile: ${JSON.stringify(context.userProfile)}
- Relevant Memories from past conversations: ${context.relevantMemories.join('\n- ')}
---
CURRENT CONVERSATION:
${context.chatHistory}
---
USER'S NEW MESSAGE: "${context.message}"

Your response (be natural and use the context above):
`;
Call the Gemini model with this prompt and return the text response.

Step 4: API Endpoint (server.js & src/routes/chat.js) (2-4 Hours)
server.js:

Set up your Express app.

Load environment variables using dotenv.

Include middleware to parse JSON bodies (express.json()) and serve static files from the public directory (express.static('public')).

Import and use your chat routes.

Start the server.

src/routes/chat.js:

Create an Express router.

Define the POST /api/chat endpoint.

Make the endpoint async. Inside, use await to call your memory and LLM handler functions in sequence.

Wrap the logic in a try...catch block for error handling.

Send the AI's response back as JSON.

Crucially, perform the memory update operations after sending the response to the user so they don't have to wait. Since Node.js is non-blocking, you can simply call these async functions without await-ing their completion before sending the res.json().

Step 5: Simple Frontend (public/) & Testing (4-6 Hours)
This step is identical to the Python plan. Create a basic HTML/CSS/JS interface that uses the fetch API to send POST requests to your /api/chat endpoint and displays the results.

Test thoroughly against all the validation criteria provided in the challenge brief.

Step 6: Documentation & Video (3-4 Hours)
This step is also identical. Focus on creating a clear README.md, a concise architecture document, and a compelling video that showcases the chatbot's memory, personality, and contextual awareness.

## âœ… How This Design Meets the Requirements
Human-Like Interaction: Achieved via careful prompt engineering in llm_handler.js.

Personalized Memory: The SQLite (facts) + LanceDB (semantic context) combo provides robust memory.

Modularity: The logic is separated into services (memory.js, llm_handler.js) and routes (chat.js), making the core logic reusable and easy to maintain.

Cost-Effectiveness: Using gemini-1.5-flash, local SQLite, and local LanceDB eliminates external service costs and keeps API token usage efficient.

Identity & Hallucination Resistance: The fixed system prompt enforces the persona, while grounding the model with retrieved data minimizes the chance of it making things up.